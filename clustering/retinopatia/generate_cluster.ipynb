{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOo/XR9TENvjyipgRnSnNoc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ya3d6-G1HmuW","executionInfo":{"status":"ok","timestamp":1690643111186,"user_tz":180,"elapsed":52525,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}},"outputId":"e8f28abe-7ad2-4434-e2af-61b786a02509"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install faiss-cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wF-EuhuhFNGq","executionInfo":{"status":"ok","timestamp":1690643117580,"user_tz":180,"elapsed":6397,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}},"outputId":"6e7ebc6f-8b2b-4740-81e1-afed6b7f4bfc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.7.4\n"]}]},{"cell_type":"code","source":["!pip install efficientnet_pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8q-EU3lKyqp","executionInfo":{"status":"ok","timestamp":1690643128076,"user_tz":180,"elapsed":10500,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}},"outputId":"304dfd7e-ca41-4b2f-df39-31c9789acd11"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting efficientnet_pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet_pytorch\n","  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=978d42edff40d1a54ce37be9fcbf8d93a1ba6b77911e349b732b2a5ccba21e87\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","Successfully built efficientnet_pytorch\n","Installing collected packages: efficientnet_pytorch\n","Successfully installed efficientnet_pytorch-0.7.1\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from efficientnet_pytorch import EfficientNet\n","\n","from PIL import Image\n","import faiss\n","import numpy as np\n","import os\n","from os.path import exists, join, isfile, realpath, isdir\n","from os import listdir, makedirs, walk\n","\n","import numpy as np\n","from scipy.spatial.distance import pdist, squareform\n","\n","import shutil"],"metadata":{"id":"c6PSiBmPFHgK","executionInfo":{"status":"ok","timestamp":1690643135673,"user_tz":180,"elapsed":7599,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["if exists('/content/drive/MyDrive/ORT/Master/Codes/datasets/transformed_datasets/retinopatia/train/'):\n","  WORK_DIR = '/content/drive/MyDrive/ORT/Master/Codes/datasets/transformed_datasets/retinopatia/train/'\n","  CLUSTER_DIR = '/content/drive/MyDrive/ORT/Tesis/Codes/clustering/retinopatia/'\n","elif exists('/content/drive/MyDrive/ORT/Tesis/Codes/datasets/transformed_datasets/retinopatia/train/'):\n","  WORK_DIR = '/content/drive/MyDrive/ORT/Tesis/Codes/datasets/transformed_datasets/retinopatia/train/'\n","  CLUSTER_DIR = '/content/drive/MyDrive/ORT/Tesis/Codes/clustering/retinopatia/'"],"metadata":{"id":"OZdMKFPUHi7M","executionInfo":{"status":"ok","timestamp":1690643136470,"user_tz":180,"elapsed":800,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def get_dir_files(dir_path: str):\n","    return [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n","\n","\n","def get_dirs(dir_path: str):\n","    return [d for d in listdir(dir_path) if isdir(join(dir_path, d))]"],"metadata":{"id":"DQb5URMiIB9j","executionInfo":{"status":"ok","timestamp":1690643136470,"user_tz":180,"elapsed":2,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgY4IJb1E3lC","executionInfo":{"status":"ok","timestamp":1690643137161,"user_tz":180,"elapsed":693,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}},"outputId":"13a1ef0a-8dcb-42e5-ce79-ac3f6975f8a2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n","100%|██████████| 20.4M/20.4M [00:00<00:00, 122MB/s] \n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b0\n"]}],"source":["\n","# cargar EfficientNet-b0 model\n","model = EfficientNet.from_pretrained('efficientnet-b0')\n","\n","# remover ultima layer\n","model._fc = torch.nn.Identity()\n","\n","model.eval()\n","\n","# transformacion imagen\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5]),\n","])\n"]},{"cell_type":"code","source":["def extract_features(img_path):\n","    # Load the image\n","    img = Image.open(img_path).convert('RGB')\n","\n","    # Apply the transformations and add an extra dimension (for the batch)\n","    img_t = transform(img)\n","    img_t = img_t.unsqueeze(0)\n","\n","    # Don't calculate gradients\n","    with torch.no_grad():\n","        # Get the features from this image\n","        features = model(img_t)\n","\n","    # The output will be in the form of a tensor, so we convert it to an array\n","    # Flatten the tensor to a 1D array\n","    features = features.numpy().flatten()\n","\n","    return features\n"],"metadata":{"id":"75VFwUQ7G9gW","executionInfo":{"status":"ok","timestamp":1690643137579,"user_tz":180,"elapsed":420,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["CLASS_NAME = get_dirs(WORK_DIR)[0]\n","IMAGE_PATH = join(WORK_DIR, CLASS_NAME)\n","image_paths = get_dir_files(IMAGE_PATH)"],"metadata":{"id":"-_f2YsqyHFua","executionInfo":{"status":"ok","timestamp":1690643141106,"user_tz":180,"elapsed":3529,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Create a directory for the clusters if it doesn't exist\n","if not os.path.exists(join(CLUSTER_DIR,f'clusters_{CLASS_NAME}_efficient')):\n","    os.makedirs(join(CLUSTER_DIR,f'clusters_{CLASS_NAME}_efficient'))"],"metadata":{"id":"lQH5q-N0IRXT","executionInfo":{"status":"ok","timestamp":1690643141563,"user_tz":180,"elapsed":459,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["%cd \"$IMAGE_PATH\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3d4YfPpJNYY","executionInfo":{"status":"ok","timestamp":1690643141563,"user_tz":180,"elapsed":2,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}},"outputId":"8ee5a5ea-eabb-4758-9257-f28bac8ebb21"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ORT/Tesis/Codes/datasets/transformed_datasets/retinopatia/train/mild\n"]}]},{"cell_type":"code","source":["# Extract features for all images\n","all_features = []\n","for image_path in image_paths:\n","    features = extract_features(image_path)\n","    all_features.append(features)\n","\n","# Convert list of features to numpy array\n","all_features = np.vstack(all_features)\n","\n","# Dimension of our vector space\n","d = all_features.shape[1]\n","\n","# Initialize a FAISS index\n","index = faiss.IndexFlatL2(d)\n","\n","# Add vectors to the index\n","index.add(all_features)"],"metadata":{"id":"wh9CFE1lHER3","executionInfo":{"status":"ok","timestamp":1690643185681,"user_tz":180,"elapsed":44119,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Number of clusters\n","k = 10\n","\n","# Faiss k-means clustering\n","niter = 30\n","verbose = True\n","kmeans = faiss.Clustering(d, k)\n","kmeans.niter = niter\n","kmeans.verbose = verbose\n","kmeans.train(all_features, index)\n","\n","# The centroids are stored in kmeans.centroids\n","centroids = faiss.vector_to_array(kmeans.centroids).reshape(k, d)\n","\n","# To assign each vector to a cluster, you can use the index.search function\n","D, I = index.search(all_features, 1)\n","\n","# I contains the cluster assignments for each vector\n","cluster_assignments = I.reshape(-1)"],"metadata":{"id":"FlN662WqL8l-","executionInfo":{"status":"ok","timestamp":1690643185681,"user_tz":180,"elapsed":6,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Create a dictionary of clusters\n","clusters = {i: [] for i in range(k)}\n","\n","# Assign each image to a cluster\n","for image_path, cluster_id in zip(image_paths, cluster_assignments):\n","    clusters[cluster_id].append(image_path)\n","\n","# Now, clusters[i] is a list of images that belong to cluster i\n","for i in range(k):\n","    print(f\"Cluster {i}:\")\n","\n","    # Create a directory for this cluster if it doesn't exist\n","    cluster_dir = join(CLUSTER_DIR,f'clusters_{CLASS_NAME}_efficient', f'cluster_{i}')\n","    if exists(cluster_dir):\n","      shutil.rmtree(cluster_dir)\n","    makedirs(cluster_dir)\n","\n","    for image_path in clusters[i]:\n","        # Copy the image into the cluster directory\n","        shutil.copy(image_path, cluster_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zko6srRUMF6u","executionInfo":{"status":"ok","timestamp":1690643194653,"user_tz":180,"elapsed":8976,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}},"outputId":"ea17fb72-71b5-4c1d-fab9-9215470deb36"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster 0:\n","Cluster 1:\n","Cluster 2:\n","Cluster 3:\n","Cluster 4:\n","Cluster 5:\n","Cluster 6:\n","Cluster 7:\n","Cluster 8:\n","Cluster 9:\n"]}]},{"cell_type":"code","source":["def compute_distances(feature_vectors, centroid):\n","    # Convert list to numpy array\n","    feature_vectors = np.vstack(feature_vectors)\n","\n","    # Compute distances\n","    distances = np.linalg.norm(feature_vectors - centroid, axis=1)\n","\n","    return distances"],"metadata":{"id":"vmLOnH6_Cpqw","executionInfo":{"status":"ok","timestamp":1690643194654,"user_tz":180,"elapsed":4,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["N = 25  # number of images you want to keep per cluster\n","\n","# Initialize dictionaries for average and standard deviation of similarities\n","average_similarities = {}\n","std_dev_similarities = {}\n","centroid_distances = {}\n","top_average_similarities = {}\n","top_std_dev_similarities = {}\n","best_clusters = {}\n","\n","for cluster_id, image_paths in clusters.items():\n","    # Extract the feature vectors for images in this cluster\n","    feature_vectors = np.array([extract_features(image_path) for image_path in image_paths])\n","\n","    # Calculate distances from centroid\n","    distances = compute_distances(feature_vectors, centroids[cluster_id])\n","\n","    # Save distances to centroid_distances dictionary\n","    centroid_distances[cluster_id] = distances\n","\n","    # Pair images with their distances and sort by distance\n","    image_distances = sorted(zip(image_paths, distances), key=lambda x: x[1])\n","\n","    # Calculate pairwise cosine similarities\n","    similarity_matrix = 1 - squareform(pdist(feature_vectors, metric='cosine'))\n","\n","    # Flatten the matrix into a list and remove self-similarities (diagonal of the matrix)\n","    similarities = similarity_matrix[np.triu_indices(similarity_matrix.shape[0], k=1)]\n","\n","    # Calculate the average and standard deviation of similarities\n","    average_similarities[cluster_id] = np.mean(similarities)\n","    std_dev_similarities[cluster_id] = np.std(similarities)\n","\n","    # Select the top N images\n","    top_images = image_distances[:N]\n","\n","    # Extract the feature vectors for the top N images in this cluster\n","    top_feature_vectors = feature_vectors[:N]\n","\n","    # Calculate pairwise cosine similarities for top images\n","    top_similarity_matrix = 1 - squareform(pdist(top_feature_vectors, metric='cosine'))\n","\n","    # Flatten the matrix into a list and remove self-similarities (diagonal of the matrix)\n","    top_similarities = top_similarity_matrix[np.triu_indices(top_similarity_matrix.shape[0], k=1)]\n","\n","    # Calculate the average and standard deviation of similarities for top images\n","    top_average_similarities[cluster_id] = np.mean(top_similarities)\n","    top_std_dev_similarities[cluster_id] = np.std(top_similarities)\n","\n","    # Check the condition for similarities and standard deviations\n","    if top_average_similarities[cluster_id] - top_std_dev_similarities[cluster_id] >= 0.65:\n","        # Only proceed with this cluster if the condition is met\n","        best_clusters[cluster_id] = True\n","\n","        # Now, copy these images to the new directory\n","        cluster_dir = join(CLUSTER_DIR, f'clusters_{CLASS_NAME}_efficient', 'best_results', f'cluster_{cluster_id}')\n","        if exists(cluster_dir):\n","          shutil.rmtree(cluster_dir)\n","        makedirs(cluster_dir)\n","        for image_path, _ in top_images:\n","            # Copy the image into the new cluster directory\n","            shutil.copy(image_path, cluster_dir)\n"],"metadata":{"id":"DyldZyUpR22w","executionInfo":{"status":"ok","timestamp":1690643226671,"user_tz":180,"elapsed":32020,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["for cluster_id in centroid_distances.keys():\n","    # Define the path to the cluster directory\n","    cluster_dir = join(CLUSTER_DIR, f'clusters_{CLASS_NAME}_efficient', 'best_results', f'cluster_{cluster_id}')\n","\n","    # Get a count of all files in the directory (assumes all files are images)\n","    image_count = len(os.listdir(cluster_dir)) if os.path.exists(cluster_dir) else 0\n","\n","    print(f\"Cluster {cluster_id}:\")\n","    print(f\"Image count: {image_count}\")\n","    print(f\"Average similarity: {top_average_similarities[cluster_id]}\")\n","    print(f\"Standard deviation of similarity: {top_std_dev_similarities[cluster_id]}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uog8VMjlC2uO","executionInfo":{"status":"ok","timestamp":1690643226672,"user_tz":180,"elapsed":4,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}},"outputId":"7c13062d-106a-4c44-92e0-4cb0314b4d54"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster 0:\n","Image count: 25\n","Average similarity: 0.8886942576646841\n","Standard deviation of similarity: 0.044125791116829995\n","\n","Cluster 1:\n","Image count: 25\n","Average similarity: 0.9354241216544406\n","Standard deviation of similarity: 0.022694040385913584\n","\n","Cluster 2:\n","Image count: 15\n","Average similarity: 0.9205824015707429\n","Standard deviation of similarity: 0.030245922968237266\n","\n","Cluster 3:\n","Image count: 19\n","Average similarity: 0.9302734992610463\n","Standard deviation of similarity: 0.022000396889910782\n","\n","Cluster 4:\n","Image count: 25\n","Average similarity: 0.9178726634536148\n","Standard deviation of similarity: 0.04044263686490279\n","\n","Cluster 5:\n","Image count: 7\n","Average similarity: 0.9002619424125406\n","Standard deviation of similarity: 0.023681112457936443\n","\n","Cluster 6:\n","Image count: 25\n","Average similarity: 0.8824425863364069\n","Standard deviation of similarity: 0.045916936696064385\n","\n","Cluster 7:\n","Image count: 13\n","Average similarity: 0.9201084646535025\n","Standard deviation of similarity: 0.03096833241746696\n","\n","Cluster 8:\n","Image count: 25\n","Average similarity: 0.9386313943496265\n","Standard deviation of similarity: 0.02329803550275602\n","\n","Cluster 9:\n","Image count: 25\n","Average similarity: 0.941478386182561\n","Standard deviation of similarity: 0.021553357144909854\n","\n"]}]},{"cell_type":"code","source":["dirclus = join(CLUSTER_DIR,f'clusters_{CLASS_NAME}_clip')\n","# Now, change the working directory\n","os.chdir(dirclus)\n","folder_to_compress='best_results'\n","zip_file=f'{CLASS_NAME}_clusters.zip'\n","!zip -r -q \"$zip_file\" \"$folder_to_compress\""],"metadata":{"id":"V6wqvt9yuyxL","executionInfo":{"status":"ok","timestamp":1690643630013,"user_tz":180,"elapsed":33465,"user":{"displayName":"Federico de Leon","userId":"01045121239102538016"}}},"execution_count":20,"outputs":[]}]}