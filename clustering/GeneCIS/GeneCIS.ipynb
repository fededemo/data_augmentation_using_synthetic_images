{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPD4o6ROOAley/4sspKn5KW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNHfAU2lWD-T","executionInfo":{"status":"ok","timestamp":1690327778211,"user_tz":300,"elapsed":18409,"user":{"displayName":"A. Mauricio Repetto","userId":"04546011941122484957"}},"outputId":"b4e2a7c6-b5cc-4d5b-9fbd-43d35eb09639"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# !pip install inflect==6.0.0\n","# !pip install matplotlib==3.5.1\n","# !pip install numpy==1.22.3\n","# !pip install pandas==1.4.2\n","# !pip install Pillow==9.4.0\n","# !pip install SceneGraphParser==0.1.0\n","# !pip install submitit==1.4.5\n","# !pip install tensorboard==2.9.1\n","# !pip install tqdm==4.64.0"],"metadata":{"id":"cD8Dvl_cWNmf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6XJs-VQZG2u","executionInfo":{"status":"ok","timestamp":1690328000343,"user_tz":300,"elapsed":8566,"user":{"displayName":"A. Mauricio Repetto","userId":"04546011941122484957"}},"outputId":"5e797cc4-22d0-4bf2-fbcd-21e406cbcca6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-js9viqqa\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-js9viqqa\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from clip==1.0)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369501 sha256=4793907e313c11d2fcd9dc39faa954bd9904e3ce5ed3525c9dd0a01751f0575e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ejqx5j12/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.1.1\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import clip\n","\n","from PIL import Image\n","\n","import numpy as np\n","\n","import os\n","from os.path import exists, join, isfile, realpath, isdir\n","from os import listdir, makedirs, walk\n","\n","import shutil as sh\n","\n","from tqdm import tqdm"],"metadata":{"id":"jWzk0qwUXrnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if exists('/content/drive/MyDrive/ORT/Master/Codes'):\n","  WORK_DIR = '/content/drive/MyDrive/ORT/Master/Codes'\n","elif exists('/content/drive/MyDrive/ORT/Tesis/Codes'):\n","  WORK_DIR = '/content/drive/MyDrive/ORT/Tesis/Codes'\n","\n","WORK_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"SgiWRCLJX280","executionInfo":{"status":"ok","timestamp":1690327850760,"user_tz":300,"elapsed":223,"user":{"displayName":"A. Mauricio Repetto","userId":"04546011941122484957"}},"outputId":"a14015d5-b676-460b-932c-3019f53754d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/ORT/Master/Codes'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["%cd {WORK_DIR}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58b3QrpPYF0S","executionInfo":{"status":"ok","timestamp":1690327864544,"user_tz":300,"elapsed":217,"user":{"displayName":"A. Mauricio Repetto","userId":"04546011941122484957"}},"outputId":"03d2d492-8ed0-4726-f171-58bb1c12b295"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1OcSwcT_BqQPziJvgS9FWGbmkXwMFmj7A/Tesis/Codes\n"]}]},{"cell_type":"code","source":["def get_dir_files(dir_path: str):\n","    return [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n","\n","\n","def get_dirs(dir_path: str):\n","    return [d for d in listdir(dir_path) if isdir(join(dir_path, d))]"],"metadata":{"id":"NZ2lphRIXwNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def strip_state_dict(state_dict: torch.nn.Module.state_dict, strip_key: str = 'module.'):\n","\n","    \"\"\"\n","    Strip strip_key from start of state_dict keys\n","    Useful if model has been trained as DDP model\n","    \"\"\"\n","\n","    for k in list(state_dict.keys()):\n","        if k.startswith(strip_key):\n","            state_dict[k[len(strip_key):]] = state_dict[k]\n","            del state_dict[k]\n","\n","    return state_dict"],"metadata":{"id":"6HoqxLangDPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# Modified by Sagar Vaze from https://github.com/ABaldrati/CLIP4CirDemo/blob/main/model.py\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","import numpy as np\n","\n","\"\"\"\n","Code from: https://github.com/ABaldrati/CLIP4CirDemo/blob/main/model.py\n","\"\"\"\n","\n","class Combiner(nn.Module):\n","    \"\"\"\n","    Combiner module which once trained fuses textual and visual information\n","    \"\"\"\n","\n","    def __init__(self, clip_feature_dim: int, projection_dim: int, hidden_dim: int):\n","        \"\"\"\n","        :param clip_feature_dim: CLIP input feature dimension\n","        :param projection_dim: projection dimension\n","        :param hidden_dim: hidden dimension\n","        \"\"\"\n","        super(Combiner, self).__init__()\n","        self.text_projection_layer = nn.Linear(clip_feature_dim, projection_dim)\n","        self.image_projection_layer = nn.Linear(clip_feature_dim, projection_dim)\n","\n","        self.dropout1 = nn.Dropout(0.5)\n","        self.dropout2 = nn.Dropout(0.5)\n","\n","        self.combiner_layer = nn.Linear(projection_dim * 2, hidden_dim)\n","        self.output_layer = nn.Linear(hidden_dim, clip_feature_dim)\n","\n","        self.dropout3 = nn.Dropout(0.5)\n","        self.dynamic_scalar = nn.Sequential(nn.Linear(projection_dim * 2, hidden_dim), nn.ReLU(), nn.Dropout(0.5),\n","                                            nn.Linear(hidden_dim, 1),\n","                                            nn.Sigmoid())\n","\n","        self.logit_scale = 100\n","\n","    @torch.jit.export\n","    def forward(self, image_features, text_features):\n","        \"\"\"\n","        Cobmine the reference image features and the caption features. It outputs the predicted features\n","        :param image_features: CLIP reference image features\n","        :param text_features: CLIP relative caption features\n","        :return: predicted features\n","        \"\"\"\n","\n","        text_projected_features = self.dropout1(F.relu(self.text_projection_layer(text_features)))\n","        image_projected_features = self.dropout2(F.relu(self.image_projection_layer(image_features)))\n","\n","        raw_combined_features = torch.cat((text_projected_features, image_projected_features), -1)\n","        combined_features = self.dropout3(F.relu(self.combiner_layer(raw_combined_features)))\n","        dynamic_scalar = self.dynamic_scalar(raw_combined_features)\n","        output = self.output_layer(combined_features) + dynamic_scalar * text_features + (\n","                1 - dynamic_scalar) * image_features\n","\n","        return F.normalize(output)"],"metadata":{"id":"-BAkXtJZfA7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set to path of model to evaluate (combiner head)  (set to 'None' if using image_only etc.)\n","COMBINER_PRETRAIN_PATH = join(WORK_DIR, \"clustering/GeneCIS/vitb16_combiner_head.pt\")\n","# Set to path of model to evaluate (backbone)  (set to 'None' to use CLIP pre-trained model, if using image_only etc.)\n","BACKBONE_PRETRAIN_PATH = join(WORK_DIR, \"clustering/GeneCIS/vitb16_backbone.pt\")"],"metadata":{"id":"5s9qTiccaNZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = \"RN50x4\" # ViT-B/16"],"metadata":{"id":"GNv34jpudstz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clip_model, preprocess = clip.load(model)\n","clip_model.float().eval()\n","input_dim = clip_model.visual.input_resolution\n","feature_dim = clip_model.visual.output_dim\n","\n","combiner = Combiner(clip_feature_dim=feature_dim, projection_dim=2560, hidden_dim=2 * 2560)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7T4eNgp7dqqd","executionInfo":{"status":"ok","timestamp":1690329480201,"user_tz":300,"elapsed":22550,"user":{"displayName":"A. Mauricio Repetto","userId":"04546011941122484957"}},"outputId":"cb7c0bbb-b7b0-46de-d656-94b4407111d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 402M/402M [00:09<00:00, 42.2MiB/s]\n"]}]},{"cell_type":"code","source":["state_dict = torch.load(COMBINER_PRETRAIN_PATH, map_location='cpu')\n","state_dict = strip_state_dict(state_dict=state_dict, strip_key='module.')\n","combiner.load_state_dict(state_dict)\n","\n","state_dict = torch.load(BACKBONE_PRETRAIN_PATH, map_location='cpu')\n","state_dict = strip_state_dict(state_dict=state_dict, strip_key='module.')\n","clip_model.load_state_dict(state_dict)\n","\n","# --------------\n","# To cuda\n","# --------------\n","clip_model, combiner = clip_model.cuda(), combiner.cuda()\n","\n","clip_model.to(\"cuda\")\n","combiner.to(\"cuda\")\n","\n","# if any([p.requires_grad for p in clip_model.parameters()]):\n","#     clip_model = CLIPDistDataParallel(clip_model, device_ids=[args.gpu])\n","# if any([p.requires_grad for p in combiner.parameters()]):\n","#     combiner = torch.nn.parallel.DistributedDataParallel(combiner, device_ids=[args.gpu])"],"metadata":{"id":"PHhlyZvEfebo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clip_model.eval()\n","combiner.eval()\n","\n"],"metadata":{"id":"5TpoNG65dKpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def validate(clip_model, combiner, valloader, topk=(1, 2, 3), save_path=None):\n","\n","    print('Computing eval with combiner...')\n","\n","    clip_model.eval()\n","    combiner.eval()\n","\n","    sims_to_save = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(valloader):\n","\n","            ref_img, caption, gallery_set, target_rank = [x.cuda(non_blocking=True) for x in batch[:4]]\n","            bsz, n_gallery, _, h, w = gallery_set.size()\n","            caption = caption.squeeze()\n","\n","            # Forward pass in CLIP\n","            imgs_ = torch.cat([ref_img, gallery_set.view(-1, 3, h, w)], dim=0)\n","            all_img_feats = clip_model.encode_image(imgs_).float()\n","            caption_feats = clip_model.encode_text(caption).float()\n","\n","            # L2 normalize and view into correct shapes\n","            ref_feats, gallery_feats = all_img_feats.split((bsz, bsz * n_gallery), dim=0)\n","            gallery_feats = gallery_feats.view(bsz, n_gallery, -1)\n","            gallery_feats = torch.nn.functional.normalize(gallery_feats, dim=-1)\n","\n","            # Forward pass in combiner\n","            combined_feats = combiner(ref_feats, caption_feats)\n","\n","            # Compute similarity\n","            similarities = combined_feats[:, None, :] * gallery_feats       # B x N x D\n","            similarities = similarities.sum(dim=-1)                         # B x N\n","\n","            # Sort the similarities in ascending order (closest example is the predicted sample)\n","            _, sort_idxs = similarities.sort(dim=-1, descending=True)                   # B x N\n","\n","            # Compute recall at K\n","            for k in topk:\n","\n","                recall_k = get_recall(sort_idxs[:, :k], target_rank)\n","                meters[k].update(recall_k, bsz)\n","\n","            sims_to_save.append(similarities.cpu())\n","\n","        if save_path is not None:\n","            sims_to_save = torch.cat(sims_to_save)\n","            print(f'Saving text only preds to: {save_path}')\n","            torch.save(sims_to_save, save_path)\n","\n","        # Print results\n","        print_str = '\\n'.join([f'Recall @ {k} = {v.avg:.4f}' for k, v in meters.items()])\n","        print(print_str)\n","\n","        return meters"],"metadata":{"id":"aF4jBSDoazyx"},"execution_count":null,"outputs":[]}]}