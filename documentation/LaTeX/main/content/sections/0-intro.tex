\chapter{Introduction}

Artificial Intelligence (AI) and deep learning model development rely heavily on the availability of large data sets. However, fields like medicine often face data scarcity due to the rarity of certain conditions or diseases, challenges in data collection, or exorbitant acquisition costs, resulting in the limited performance of AI models~\cite{RDopportunities}. This research attempts to overcome these limitations by generating synthetic data via contemporary generative AI techniques such as diffusion models. These techniques offer an alternative to more traditional data augmentation procedures, which have been extensively discussed in various studies, one example of this is Mingle Xu et al.~\cite{augmentation_techniques}, typically involving data transformations such as rotation, zooming, cropping, among others.  
    
Synthetic data, as opposed to real-world data, is artificially generated, mimicking real-world data's statistical properties and correlations. While the best insights come from real data, it's often expensive, imbalanced, unavailable, or unusable due to privacy regulations or other factors. Hence, synthetic data can be an effective complement or alternative and when merged with real data, it can create enhanced datasets that address the gaps and weaknesses often found in real data and improve AI models by increasing robustness and generalizability.

In addition to our work, there has been a growing interest in the use of synthetic data for ML, Gartner in 2021 predicted that "\textit{by 2030, most of the data used in AI will be artificially generated by rules, statistical models, simulations or other techniques}"~\cite{gartnerMaverickResearch} and later the same year it made another prediction saying "\textit{by 2024, 60\% of the data used for the development of AI and analytics projects will be synthetically generated}"~\cite{wsjFakeMake}. These predictions suggest that synthetic data is a rapidly growing field with the potential to revolutionize the way we develop and train ML models, showing the importance to make advances in the subject. Proof of this trend is evident in the growing emergence of platforms dedicated solely to the process of generating synthetic data, such as \textit{Gretel}~\cite{gretel} and more recently \textit{MONAI}~\cite{monai}.

In collaboration with Cogniflow~\cite{cogniflow}, a No-Code AI platform with AutoML capabilities, this project pursues enhancing the performance of various image classification models by applying generative techniques. Through research and analysis of the state of the art (SOTA) in synthetic data generation, we developed and tested different models to address problems where the lack of training data or the imbalance between classes negatively affects the performance of deep learning models.
    
This project is carried out using public datasets, and although the available datasets focus on the health area, the results and applications of this study will not be limited to this industry. The research's findings are available to the scientific community, and eventually, they can be integrated into the Cogniflow platform to enhance its ML service offerings.

The project methodology included the following steps:
    
\begin{itemize}
    \item Studying the SOTA in synthetic data generation.
    \item Analyzing the available data and information.
    \item Defining evaluation metrics to compare results.
    \item Generating different models based on the studied techniques.
    \item Developed small proof of concepts (PoC) that demonstrates the complete training process for a given dataset.
    \item Presenting the conclusions and lessons learned throughout the project.
\end{itemize}

This document provides a detailed description of the project, its objectives, expected outcomes, and technological features, including the use of neural network models, GANs, diffusion models, cloud computing, and various related tools and technologies.

The project was expected to take 6 months to complete where the major milestones along the way included:

\begin{itemize}
    \item \textbf{Month 1-2}: Complete the literature review, check available models and techniques, among others and develop a research plan.
    \item \textbf{Month 3-6}:
    \begin{itemize}
    \item Collect and preprocess the data.
    \item Implement and train the models.
        \end{itemize}
    \item \textbf{Month 6}: Evaluate the models and present the findings.
\end{itemize}

The project faced a number of challenges, including:

\begin{itemize}
    \item The complexity of the generative models.
    \item The diversity of data between datasets.
    \item The availability of existing generative models.
    \item The computing resources needed to train or finetune such models.
    \item The potential for bias in the generated data.
\end{itemize}

We managed to mitigate some of these challenges and eventually, we hope that the insights presented in this work contribute to the broader research community and future endeavors in synthetic data generation.